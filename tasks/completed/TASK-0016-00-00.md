# TASK-0016-00-00: Deterministic Output Metric Naming

**Status**: [ ] Not Started | [X] In Progress | [ ] Blocked | [ ] Complete | [ ] Abandoned
**Created**: 2025-01-08
**Updated**: 2025-01-08
**Assignee**: [Unassigned]
**Priority**: P1 (High)
**Parent Task**: None
**Dependencies**: None
**Estimated Effort**: M (1d)

## User Story

As a metrics processor user,
I want to define custom output metric names using patterns,
So that I can have concise, meaningful metric names regardless of the number of inputs and outputs.

## Context & Research

### Current State Analysis
- [ ] Review current metric naming implementation in processor
- [ ] Document existing naming patterns (single input vs multi-input)
- [ ] Identify limitations of current approach
- [ ] Analyze impact on existing configurations

### Current Naming Behavior
- Single input: `{input_name}.{output_name}`
- Multiple inputs: `{first_input}_multi.{output_name}`
- Problems:
  - Names can become very long with multiple namespaced inputs
  - `_multi` suffix is not descriptive
  - No user control over naming pattern
  - Examples of current problems:
    - `system.memory.utilization_multi.cpu_prediction` (too long)
    - `system.cpu.utilization.prediction` (redundant namespace)

### Technical Research
- [ ] Research metric naming best practices in observability
- [ ] Review how other processors handle output naming
- [ ] Evaluate regex vs template approaches
- [ ] Consider backward compatibility requirements

## Acceptance Criteria

### Functional Requirements
- [ ] Users can specify custom output metric naming patterns in rules
- [ ] Patterns support variable substitution (inputs, outputs, model info)
- [ ] Default behavior uses smart stem extraction for concise names
- [ ] Invalid patterns produce clear error messages
- [ ] Documentation includes comprehensive examples

### Non-Functional Requirements
- [ ] No performance degradation from naming logic
- [ ] Pattern evaluation is secure (no code injection)
- [ ] Configuration validation at startup
- [ ] Clear documentation of breaking changes

## Behavioral Specifications

```gherkin
Feature: Deterministic Output Metric Naming
  As a metrics processor user
  I want to define custom output metric names
  So that I have control over metric naming conventions

  Background:
    Given the metrics inference processor is configured
    And a model "cpu-predictor" with outputs ["prediction", "variance"]

  Scenario: Default naming behavior (smart stems)
    Given a rule with single input "system.cpu.utilization"
    And no output_pattern specified
    When the processor creates output metrics
    Then the metric names should be "cpu_utilization.prediction"
    And "cpu_utilization.variance"

  Scenario: Custom pattern with single input
    Given a rule with single input "system.cpu.utilization"
    And output_pattern "cpu.{output}"
    When the processor creates output metrics
    Then the metric names should be "cpu.prediction"
    And "cpu.variance"

  Scenario: Custom pattern with multiple inputs
    Given a rule with inputs ["system.memory.utilization", "system.cpu.load"]
    And output_pattern "{model}.{output}"
    When the processor creates output metrics
    Then the metric names should be "cpu-predictor.prediction"
    And "cpu-predictor.variance"

  Scenario: Pattern with input extraction using regex
    Given a rule with input "system.cpu.utilization"
    And output_pattern "{input:cpu|memory|disk}.enhanced.{output}"
    When the processor creates output metrics
    Then the metric names should be "cpu.enhanced.prediction"
    And "cpu.enhanced.variance"

  Scenario: Invalid pattern handling
    Given a rule with invalid output_pattern "{undefined_var}.{output}"
    When the processor validates configuration
    Then it should return error "undefined variable: undefined_var"
    And the processor should not start
```

## Implementation Plan

### Phase 1: Design & Research
1. [ ] Define pattern syntax (template vs regex)
2. [ ] Create pattern variable specification
3. [ ] Design validation logic
4. [ ] Plan backward compatibility approach

### Phase 2: Development
1. [ ] Add output_pattern field to rule configuration
2. [ ] Implement pattern parser and validator
3. [ ] Create pattern evaluation engine
4. [ ] Update metric creation logic
5. [ ] Add configuration validation

### Phase 3: Testing
1. [ ] Unit tests for pattern parser
2. [ ] Unit tests for pattern evaluation
3. [ ] Integration tests with various patterns
4. [ ] Backward compatibility tests
5. [ ] Performance benchmarks

### Phase 4: Documentation
1. [ ] Update processor configuration docs
2. [ ] Create pattern syntax reference
3. [ ] Add example configurations
4. [ ] Update migration guide

## Pattern Syntax Proposal

### Variables
- `{input}` - First input metric name
- `{input[N]}` - Nth input metric name (0-based)
- `{input:regex}` - Extract from first input using regex
- `{output}` - Current output tensor name
- `{model}` - Model name
- `{version}` - Model version

### Examples
```yaml
# Simple replacement
output_pattern: "ml.{model}.{output}"
# Result: ml.cpu-predictor.prediction

# Extract component from input
output_pattern: "{input:system\\.(\\w+)}.predicted.{output}"
# Input: system.cpu.utilization
# Result: cpu.predicted.prediction

# Multiple inputs - use specific input
output_pattern: "{input[0]}.enriched_by_{model}"
# Result: system.memory.utilization.enriched_by_cpu-predictor

# Conditional patterns (future enhancement)
output_pattern: "{input =~ /cpu/ ? 'compute' : 'other'}.{output}"
```

## Test Plan

### Unit Tests
- [ ] Pattern parser: syntax validation
- [ ] Pattern evaluator: variable substitution
- [ ] Regex extraction: edge cases
- [ ] Error handling: invalid patterns

### Integration Tests
- [ ] Single input patterns
- [ ] Multi-input patterns
- [ ] Complex regex patterns
- [ ] Backward compatibility

### E2E Tests
- [ ] Full pipeline with custom patterns
- [ ] Migration from old to new config
- [ ] Performance with complex patterns

## Definition of Done
- [ ] Pattern syntax implemented and tested
- [ ] Configuration validation complete
- [ ] All tests passing
- [ ] Documentation updated
- [ ] Examples provided
- [ ] No performance regression
- [ ] Backward compatibility maintained
- [ ] PR reviewed and merged