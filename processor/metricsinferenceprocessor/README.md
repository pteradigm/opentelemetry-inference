# Metrics Inference Processor

<!-- status autogenerated section -->
| Status        |           |
| ------------- |-----------|
| Stability     | [development]: metrics   |
| Distributions | [contrib] |
| Issues        | [![Open issues](https://img.shields.io/github/issues-search/open-telemetry/opentelemetry-collector-contrib?query=is%3Aissue%20is%3Aopen%20label%3Aprocessor%2Fmetricsinference%20&label=open&color=orange&logo=opentelemetry)](https://github.com/open-telemetry/opentelemetry-collector-contrib/issues?q=is%3Aopen+is%3Aissue+label%3Aprocessor%2Fmetricsinference) [![Closed issues](https://img.shields.io/github/issues-search/open-telemetry/opentelemetry-collector-contrib?query=is%3Aissue%20is%3Aclosed%20label%3Aprocessor%2Fmetricsinference%20&label=closed&color=blue&logo=opentelemetry)](https://github.com/open-telemetry/opentelemetry-collector-contrib/issues?q=is%3Aclosed+is%3Aissue+label%3Aprocessor%2Fmetricsinference) |
| [Code Owners](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CONTRIBUTING.md#becoming-a-code-owner)    | [@rbellamy](https://www.github.com/rbellamy) |

[development]: https://github.com/open-telemetry/opentelemetry-collector/blob/main/docs/component-stability.md#development
[contrib]: https://github.com/open-telemetry/opentelemetry-collector-releases/tree/main/distributions/otelcol-contrib
<!-- end autogenerated section -->

**Status: under development; Not recommended for production usage.**

## Description

The Metrics Inference Processor (`metricsinferenceprocessor`) enables real-time machine learning inference on OpenTelemetry metrics by integrating with external inference servers using the KServe v2 inference protocol. It processes metrics through ML models and creates new metrics from the inference results.

## Key Features

### 1. Real-time ML Inference
- Processes OpenTelemetry metrics through ML models in real-time
- Supports any inference server that implements the KServe v2 protocol
- Automatically converts metrics to inference tensors and back to metrics

### 2. Automatic Metadata Discovery
- Queries model metadata during startup to discover output specifications
- Eliminates the need for manual output configuration in most cases
- Supports fallback to explicit output configuration when needed

### 3. Error Handling

- All errors are logged with appropriate context including model names and error details
- Processing continues gracefully on errors - input metrics always pass through unchanged
- Failed inference requests do not create output metrics, but do not block the pipeline
- The processor never drops or modifies input metrics regardless of inference success

### 4. Broadcast Semantics

- Intelligently handles multi-input scenarios where inputs have different attribute schemas
- Automatically broadcasts single-valued inputs to all attribute combinations of multi-valued inputs
- Ensures proper data lineage and maintains metric cardinality
- Example: Memory calculation where utilization has state labels but limit doesn't

### 5. Attribute Preservation and Namespacing

- Output metrics inherit attributes from all input metrics with automatic namespacing
- Attributes are namespaced by input metric name to prevent conflicts
- Example: `cpu` attribute from `system.cpu.utilization` becomes `system.cpu.utilization.cpu`
- Preserves resource and scope metadata from input context
- Maintains clear data lineage through the inference pipeline
- Correctly maps tensor output values to their corresponding input attributes

### 6. Rule-Based Processing

- Configure multiple rules to route different metrics to different models
- Support for parameters to customize model behavior
- Flexible input selection with label selectors

### 7. Model Input Validation

- Validates rule inputs against model signature using KServe v2 ModelMetadata
- Checks input count, data types, and tensor shapes for compatibility
- Automatically skips validation for models without metadata or input specifications
- Provides intelligent type compatibility (e.g., INT64 metrics can be used for FP64 tensors)
- Gracefully handles validation failures by continuing processing without inference

### 8. Intelligent Output Metric Naming

- Automatically generates meaningful names for output metrics
- Detects and removes common prefixes to avoid redundancy
- Groups metrics by category when dealing with many inputs
- Supports custom pattern-based naming when needed
- Ensures unique names even when using the same model multiple times

## Configuration

```yaml
processors:
  metricsinference:
    # gRPC connection settings for the inference server
    grpc:
      endpoint: "localhost:8081"
      use_ssl: false
      compression: true
    
    # Timeout for inference requests (default: 30s)
    timeout: 30
    
    # Naming configuration for output metrics (optional)
    naming:
      max_stem_parts: 2          # Maximum parts to keep from each input
      skip_common_domains: true  # Skip common prefixes like "system", "app"
      enable_category_grouping: true  # Group by categories when many inputs
      abbreviation_threshold: 4  # Number of inputs before abbreviation
    
    # Data handling configuration for real-time processing (optional)
    data_handling:
      mode: "latest"             # How to handle data points: "latest", "window", "all"
      window_size: 5             # Number of points when mode is "window"
      align_timestamps: true     # Ensure temporal alignment across inputs
      timestamp_tolerance: 1000  # Max time difference in ms for alignment
    
    # Inference rules - process metrics through ML models
    rules:
      # Simple scaling model example
      - model_name: "simple-scaler"
        model_version: "v1.0"  # Optional
        inputs: ["system.cpu.utilization"]
        # outputs: []  # Optional - auto-discovered from model metadata
        parameters:  # Optional - model-specific parameters
          scale_factor: 2.0
      
      # Multi-input model example 
      - model_name: "memory-calculator"
        inputs: 
          - "system.memory.utilization"
          - "system.memory.limit"
        parameters:
          operation: "multiply"
      
      # Rule with custom output pattern
      - model_name: "predictor"
        inputs: ["cpu.usage"]
        output_pattern: "custom.{model}.{output}"  # Optional custom naming
```

### Configuration Parameters

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `grpc.endpoint` | string | Yes | gRPC endpoint of the inference server |
| `grpc.use_ssl` | bool | No | Enable SSL/TLS for gRPC connection (default: false) |
| `grpc.compression` | bool | No | Enable gRPC compression (default: true) |
| `timeout` | int | No | Timeout for inference requests in seconds (default: 30) |
| `naming` | NamingConfig | No | Configuration for output metric naming (see below) |
| `data_handling` | DataHandlingConfig | No | Configuration for data point processing (see below) |
| `rules` | []Rule | Yes | List of inference rules |

### Naming Configuration

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `naming.max_stem_parts` | int | No | Maximum parts to keep from each input (default: 2) |
| `naming.skip_common_domains` | bool | No | Skip common prefixes (default: true) |
| `naming.enable_category_grouping` | bool | No | Group by categories (default: true) |
| `naming.abbreviation_threshold` | int | No | Inputs before abbreviation (default: 4) |

### Data Handling Configuration

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `data_handling.mode` | string | No | Data point handling mode: "latest", "window", or "all" (default: "latest") |
| `data_handling.window_size` | int | No | Number of recent points to send when mode is "window" (default: 1) |
| `data_handling.align_timestamps` | bool | No | Enable temporal alignment across inputs (default: true) |
| `data_handling.timestamp_tolerance` | int64 | No | Max time difference in ms for alignment (default: 1000) |

**Data Handling Modes:**

- **`latest`** (default): Send only the most recent data point for real-time processing
- **`window`**: Send the last N data points (sliding window) as configured by window_size
- **`all`**: Send all accumulated data points (batch processing, original behavior)

### Rule Configuration

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `model_name` | string | Yes | Name of the model on the inference server |
| `model_version` | string | No | Version of the model (server default if not specified) |
| `inputs` | []string | Yes | List of input metric names or label selectors |
| `outputs` | []OutputSpec | No | Output specifications (auto-discovered if not provided) |
| `output_pattern` | string | No | Custom naming pattern (overrides global naming config) |
| `parameters` | map | No | Model-specific parameters sent with inference requests |

### Output Specification

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `name` | string | Yes | Name for the output metric |
| `datatype` | string | No | Expected tensor data type (FP32, FP64, INT32, etc.) |
| `description` | string | No | Description for the output metric |
| `unit` | string | No | Unit for the output metric |

## Supported Inference Servers

The processor works with any server implementing the KServe v2 inference protocol:

- **MLServer** (recommended for development)
- **NVIDIA Triton Inference Server**
- **TensorFlow Serving** (with KServe v2 adapter)
- **TorchServe** (with KServe v2 adapter)
- **Custom implementations** of the KServe v2 gRPC protocol

## Example Use Cases

### 1. Anomaly Detection

```yaml
- model_name: "cpu-anomaly-detector"
  inputs: ["system.cpu.utilization"]
  # Output: cpu_utilization.anomaly_score (intelligent naming removes "system" prefix)
  # Validation: Ensures model expects 1 input with compatible data type
```

### 2. Resource Calculation with Broadcast Semantics

```yaml
- model_name: "memory-calculator"
  inputs: 
    - "system.memory.utilization"  # Has state labels (used, free, cached, etc.)
    - "system.memory.limit"        # Single value, no state label
  # Output: Calculates actual memory usage for each state
  # Output attributes include namespaced inputs:
  # memory_utilization_memory_limit.product_result{
  #   system.memory.utilization.state="used",
  #   otel.inference.model.name="memory-calculator"
  # } = utilization * limit
  # Validation: Ensures model expects exactly 2 inputs with correct types
```

### 3. Performance Optimization

```yaml
- model_name: "response-time-optimizer"
  inputs: ["http.server.duration"]
  parameters:
    optimization_target: "p95"
  # Output: server_duration.optimized_threshold (intelligent naming simplifies)
  # Validation: Checks that single input type is compatible with model's expected tensor type
```

## Monitoring and Observability

The processor logs all errors and processing information for monitoring:

- **Error Logs**: All inference failures are logged with model name, rule index, and error details
- **Debug Logs**: Detailed processing information available when debug logging is enabled
- **Metric Throughput**: Monitor via standard OpenTelemetry Collector metrics
- **Latency**: Track processing time through collector pipeline metrics

Example collector metrics queries:

```promql
# Monitor processor throughput
rate(otelcol_processor_accepted_metric_points{processor="metricsinference"}[5m])

# Track processing errors
rate(otelcol_processor_refused_metric_points{processor="metricsinference"}[5m])

# Monitor batch processing latency
histogram_quantile(0.95, rate(otelcol_processor_batch_batch_send_size_bucket{processor="metricsinference"}[5m]))
```

## Troubleshooting

### Common Issues

1. **"inference server health check failed"**
   - Verify the inference server is running and accessible
   - Check the gRPC endpoint configuration
   - Ensure the server implements KServe v2 protocol

2. **"no input metrics found for inference rule"**
   - Verify metric names in the `inputs` configuration
   - Check that input metrics exist in the data pipeline
   - Use the debug exporter to inspect available metrics

3. **"failed to query metadata for model"**
   - Ensure the model exists on the inference server
   - Verify model name and version are correct
   - Metadata discovery failure doesn't prevent inference (warning only)

4. **"input validation failed for model"**
   - Check that the number of inputs in your rule matches what the model expects
   - Verify data type compatibility between your metrics and model's expected inputs
   - Ensure your metrics have appropriate cardinality for model's tensor shape requirements
   - Validation failures are logged but don't stop metric processing (inference is skipped)

### Debug Configuration

```yaml
exporters:
  debug:
    verbosity: detailed

service:
  pipelines:
    metrics:
      receivers: [...]
      processors: [metricsinference, ...]
      exporters: [..., debug]  # Add debug exporter to see processed metrics
```

## Performance Considerations

- **Cardinality**: The processor uses minimal labels to maintain low cardinality
- **Latency**: Inference requests are processed concurrently per rule
- **Throughput**: Consider model server capacity when configuring multiple rules
- **Memory**: Output metrics are created in addition to input metrics (not replaced)
- **Validation**: Model input validation adds minimal overhead during startup and is cached per model

## Metric Naming

The processor includes an intelligent naming system that automatically generates meaningful output metric names. Key features:

- **Automatic prefix removal**: Removes redundant prefixes like "system", "app" for cleaner names
- **Semantic preservation**: Keeps the most meaningful parts of metric names
- **Category grouping**: Groups similar metrics (e.g., cpu, memory, disk) when dealing with many inputs
- **Smart abbreviation**: Abbreviates long names while maintaining readability
- **Customizable**: Configure naming behavior globally or per-rule

For detailed naming configuration and examples, see [README_naming.md](./README_naming.md).

## Development and Testing

See the project's main README for development setup and testing instructions.
