// Copyright 2022 The KServe Authors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//    http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.6
// 	protoc        (unknown)
// source: proto/v2/inference.proto

package proto

import (
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"

	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

type ServerLiveRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ServerLiveRequest) Reset() {
	*x = ServerLiveRequest{}
	mi := &file_proto_v2_inference_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ServerLiveRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ServerLiveRequest) ProtoMessage() {}

func (x *ServerLiveRequest) ProtoReflect() protoreflect.Message {
	mi := &file_proto_v2_inference_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ServerLiveRequest.ProtoReflect.Descriptor instead.
func (*ServerLiveRequest) Descriptor() ([]byte, []int) {
	return file_proto_v2_inference_proto_rawDescGZIP(), []int{0}
}

type ServerLiveResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// True if the inference server is live, false if not live.
	Live          bool `protobuf:"varint,1,opt,name=live,proto3" json:"live,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ServerLiveResponse) Reset() {
	*x = ServerLiveResponse{}
	mi := &file_proto_v2_inference_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ServerLiveResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ServerLiveResponse) ProtoMessage() {}

func (x *ServerLiveResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_v2_inference_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ServerLiveResponse.ProtoReflect.Descriptor instead.
func (*ServerLiveResponse) Descriptor() ([]byte, []int) {
	return file_proto_v2_inference_proto_rawDescGZIP(), []int{1}
}

func (x *ServerLiveResponse) GetLive() bool {
	if x != nil {
		return x.Live
	}
	return false
}

type ServerReadyRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ServerReadyRequest) Reset() {
	*x = ServerReadyRequest{}
	mi := &file_proto_v2_inference_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ServerReadyRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ServerReadyRequest) ProtoMessage() {}

func (x *ServerReadyRequest) ProtoReflect() protoreflect.Message {
	mi := &file_proto_v2_inference_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ServerReadyRequest.ProtoReflect.Descriptor instead.
func (*ServerReadyRequest) Descriptor() ([]byte, []int) {
	return file_proto_v2_inference_proto_rawDescGZIP(), []int{2}
}

type ServerReadyResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// True if the inference server is ready, false if not ready.
	Ready         bool `protobuf:"varint,1,opt,name=ready,proto3" json:"ready,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ServerReadyResponse) Reset() {
	*x = ServerReadyResponse{}
	mi := &file_proto_v2_inference_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ServerReadyResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ServerReadyResponse) ProtoMessage() {}

func (x *ServerReadyResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_v2_inference_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ServerReadyResponse.ProtoReflect.Descriptor instead.
func (*ServerReadyResponse) Descriptor() ([]byte, []int) {
	return file_proto_v2_inference_proto_rawDescGZIP(), []int{3}
}

func (x *ServerReadyResponse) GetReady() bool {
	if x != nil {
		return x.Ready
	}
	return false
}

type ModelReadyRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The name of the model to check for readiness.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// The version of the model to check for readiness. If not given the
	// server will choose a version based on the model and internal policy.
	Version       string `protobuf:"bytes,2,opt,name=version,proto3" json:"version,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ModelReadyRequest) Reset() {
	*x = ModelReadyRequest{}
	mi := &file_proto_v2_inference_proto_msgTypes[4]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ModelReadyRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ModelReadyRequest) ProtoMessage() {}

func (x *ModelReadyRequest) ProtoReflect() protoreflect.Message {
	mi := &file_proto_v2_inference_proto_msgTypes[4]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ModelReadyRequest.ProtoReflect.Descriptor instead.
func (*ModelReadyRequest) Descriptor() ([]byte, []int) {
	return file_proto_v2_inference_proto_rawDescGZIP(), []int{4}
}

func (x *ModelReadyRequest) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *ModelReadyRequest) GetVersion() string {
	if x != nil {
		return x.Version
	}
	return ""
}

type ModelReadyResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// True if the model is ready, false if not ready.
	Ready         bool `protobuf:"varint,1,opt,name=ready,proto3" json:"ready,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ModelReadyResponse) Reset() {
	*x = ModelReadyResponse{}
	mi := &file_proto_v2_inference_proto_msgTypes[5]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ModelReadyResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ModelReadyResponse) ProtoMessage() {}

func (x *ModelReadyResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_v2_inference_proto_msgTypes[5]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ModelReadyResponse.ProtoReflect.Descriptor instead.
func (*ModelReadyResponse) Descriptor() ([]byte, []int) {
	return file_proto_v2_inference_proto_rawDescGZIP(), []int{5}
}

func (x *ModelReadyResponse) GetReady() bool {
	if x != nil {
		return x.Ready
	}
	return false
}

type ServerMetadataRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ServerMetadataRequest) Reset() {
	*x = ServerMetadataRequest{}
	mi := &file_proto_v2_inference_proto_msgTypes[6]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ServerMetadataRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ServerMetadataRequest) ProtoMessage() {}

func (x *ServerMetadataRequest) ProtoReflect() protoreflect.Message {
	mi := &file_proto_v2_inference_proto_msgTypes[6]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ServerMetadataRequest.ProtoReflect.Descriptor instead.
func (*ServerMetadataRequest) Descriptor() ([]byte, []int) {
	return file_proto_v2_inference_proto_rawDescGZIP(), []int{6}
}

type ServerMetadataResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The server name.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// The server version.
	Version string `protobuf:"bytes,2,opt,name=version,proto3" json:"version,omitempty"`
	// The extensions supported by the server.
	Extensions    []string `protobuf:"bytes,3,rep,name=extensions,proto3" json:"extensions,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ServerMetadataResponse) Reset() {
	*x = ServerMetadataResponse{}
	mi := &file_proto_v2_inference_proto_msgTypes[7]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ServerMetadataResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ServerMetadataResponse) ProtoMessage() {}

func (x *ServerMetadataResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_v2_inference_proto_msgTypes[7]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ServerMetadataResponse.ProtoReflect.Descriptor instead.
func (*ServerMetadataResponse) Descriptor() ([]byte, []int) {
	return file_proto_v2_inference_proto_rawDescGZIP(), []int{7}
}

func (x *ServerMetadataResponse) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *ServerMetadataResponse) GetVersion() string {
	if x != nil {
		return x.Version
	}
	return ""
}

func (x *ServerMetadataResponse) GetExtensions() []string {
	if x != nil {
		return x.Extensions
	}
	return nil
}

type ModelMetadataRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The name of the model.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// The version of the model to check for readiness. If not given the
	// server will choose a version based on the model and internal policy.
	Version       string `protobuf:"bytes,2,opt,name=version,proto3" json:"version,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ModelMetadataRequest) Reset() {
	*x = ModelMetadataRequest{}
	mi := &file_proto_v2_inference_proto_msgTypes[8]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ModelMetadataRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ModelMetadataRequest) ProtoMessage() {}

func (x *ModelMetadataRequest) ProtoReflect() protoreflect.Message {
	mi := &file_proto_v2_inference_proto_msgTypes[8]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ModelMetadataRequest.ProtoReflect.Descriptor instead.
func (*ModelMetadataRequest) Descriptor() ([]byte, []int) {
	return file_proto_v2_inference_proto_rawDescGZIP(), []int{8}
}

func (x *ModelMetadataRequest) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *ModelMetadataRequest) GetVersion() string {
	if x != nil {
		return x.Version
	}
	return ""
}

type ModelMetadataResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The model name.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// The versions of the model available on the server.
	Versions []string `protobuf:"bytes,2,rep,name=versions,proto3" json:"versions,omitempty"`
	// The model's platform. See Platforms.
	Platform string `protobuf:"bytes,3,opt,name=platform,proto3" json:"platform,omitempty"`
	// The model's inputs.
	Inputs []*ModelMetadataResponse_TensorMetadata `protobuf:"bytes,4,rep,name=inputs,proto3" json:"inputs,omitempty"`
	// The model's outputs.
	Outputs       []*ModelMetadataResponse_TensorMetadata `protobuf:"bytes,5,rep,name=outputs,proto3" json:"outputs,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ModelMetadataResponse) Reset() {
	*x = ModelMetadataResponse{}
	mi := &file_proto_v2_inference_proto_msgTypes[9]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ModelMetadataResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ModelMetadataResponse) ProtoMessage() {}

func (x *ModelMetadataResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_v2_inference_proto_msgTypes[9]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ModelMetadataResponse.ProtoReflect.Descriptor instead.
func (*ModelMetadataResponse) Descriptor() ([]byte, []int) {
	return file_proto_v2_inference_proto_rawDescGZIP(), []int{9}
}

func (x *ModelMetadataResponse) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *ModelMetadataResponse) GetVersions() []string {
	if x != nil {
		return x.Versions
	}
	return nil
}

func (x *ModelMetadataResponse) GetPlatform() string {
	if x != nil {
		return x.Platform
	}
	return ""
}

func (x *ModelMetadataResponse) GetInputs() []*ModelMetadataResponse_TensorMetadata {
	if x != nil {
		return x.Inputs
	}
	return nil
}

func (x *ModelMetadataResponse) GetOutputs() []*ModelMetadataResponse_TensorMetadata {
	if x != nil {
		return x.Outputs
	}
	return nil
}

type ModelInferRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The name of the model to use for inferencing.
	ModelName string `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	// The version of the model to use for inference. If not given the
	// server will choose a version based on the model and internal policy.
	ModelVersion string `protobuf:"bytes,2,opt,name=model_version,json=modelVersion,proto3" json:"model_version,omitempty"`
	// Optional identifier for the request. If specified will be
	// returned in the response.
	Id string `protobuf:"bytes,3,opt,name=id,proto3" json:"id,omitempty"`
	// Optional inference parameters.
	Parameters map[string]*InferParameter `protobuf:"bytes,4,rep,name=parameters,proto3" json:"parameters,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// The input tensors for the inference.
	Inputs []*ModelInferRequest_InferInputTensor `protobuf:"bytes,5,rep,name=inputs,proto3" json:"inputs,omitempty"`
	// The requested output tensors for the inference. Optional, if not
	// specified all outputs produced by the model will be returned.
	Outputs []*ModelInferRequest_InferRequestedOutputTensor `protobuf:"bytes,6,rep,name=outputs,proto3" json:"outputs,omitempty"`
	// The data contained in an input tensor can be represented in "raw"
	// bytes form or in the repeated type that matches the tensor's data
	// type. To use the raw representation 'raw_input_contents' must be
	// initialized with data for each tensor in the same order as
	// 'inputs'. For each tensor, the size of this content must match
	// what is expected by the tensor's shape and data type. The raw
	// data must be the flattened, one-dimensional, row-major order of
	// the tensor elements without any stride or padding between the
	// elements. Note that the FP16 and BF16 data types must be represented as
	// raw content as there is no specific data type for a 16-bit float type.
	//
	// If this field is specified then InferInputTensor::contents must
	// not be specified for any input tensor.
	RawInputContents [][]byte `protobuf:"bytes,7,rep,name=raw_input_contents,json=rawInputContents,proto3" json:"raw_input_contents,omitempty"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *ModelInferRequest) Reset() {
	*x = ModelInferRequest{}
	mi := &file_proto_v2_inference_proto_msgTypes[10]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ModelInferRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ModelInferRequest) ProtoMessage() {}

func (x *ModelInferRequest) ProtoReflect() protoreflect.Message {
	mi := &file_proto_v2_inference_proto_msgTypes[10]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ModelInferRequest.ProtoReflect.Descriptor instead.
func (*ModelInferRequest) Descriptor() ([]byte, []int) {
	return file_proto_v2_inference_proto_rawDescGZIP(), []int{10}
}

func (x *ModelInferRequest) GetModelName() string {
	if x != nil {
		return x.ModelName
	}
	return ""
}

func (x *ModelInferRequest) GetModelVersion() string {
	if x != nil {
		return x.ModelVersion
	}
	return ""
}

func (x *ModelInferRequest) GetId() string {
	if x != nil {
		return x.Id
	}
	return ""
}

func (x *ModelInferRequest) GetParameters() map[string]*InferParameter {
	if x != nil {
		return x.Parameters
	}
	return nil
}

func (x *ModelInferRequest) GetInputs() []*ModelInferRequest_InferInputTensor {
	if x != nil {
		return x.Inputs
	}
	return nil
}

func (x *ModelInferRequest) GetOutputs() []*ModelInferRequest_InferRequestedOutputTensor {
	if x != nil {
		return x.Outputs
	}
	return nil
}

func (x *ModelInferRequest) GetRawInputContents() [][]byte {
	if x != nil {
		return x.RawInputContents
	}
	return nil
}

type ModelInferResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The name of the model used for inference.
	ModelName string `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	// The version of the model used for inference.
	ModelVersion string `protobuf:"bytes,2,opt,name=model_version,json=modelVersion,proto3" json:"model_version,omitempty"`
	// The id of the inference request if one was specified.
	Id string `protobuf:"bytes,3,opt,name=id,proto3" json:"id,omitempty"`
	// Optional inference response parameters.
	Parameters map[string]*InferParameter `protobuf:"bytes,4,rep,name=parameters,proto3" json:"parameters,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// The output tensors holding inference results.
	Outputs []*ModelInferResponse_InferOutputTensor `protobuf:"bytes,5,rep,name=outputs,proto3" json:"outputs,omitempty"`
	// The data contained in an output tensor can be represented in
	// "raw" bytes form or in the repeated type that matches the
	// tensor's data type. To use the raw representation 'raw_output_contents'
	// must be initialized with data for each tensor in the same order as
	// 'outputs'. For each tensor, the size of this content must match
	// what is expected by the tensor's shape and data type. The raw
	// data must be the flattened, one-dimensional, row-major order of
	// the tensor elements without any stride or padding between the
	// elements. Note that the FP16 and BF16 data types must be represented as
	// raw content as there is no specific data type for a 16-bit float type.
	//
	// If this field is specified then InferOutputTensor::contents must
	// not be specified for any output tensor.
	RawOutputContents [][]byte `protobuf:"bytes,6,rep,name=raw_output_contents,json=rawOutputContents,proto3" json:"raw_output_contents,omitempty"`
	unknownFields     protoimpl.UnknownFields
	sizeCache         protoimpl.SizeCache
}

func (x *ModelInferResponse) Reset() {
	*x = ModelInferResponse{}
	mi := &file_proto_v2_inference_proto_msgTypes[11]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ModelInferResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ModelInferResponse) ProtoMessage() {}

func (x *ModelInferResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_v2_inference_proto_msgTypes[11]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ModelInferResponse.ProtoReflect.Descriptor instead.
func (*ModelInferResponse) Descriptor() ([]byte, []int) {
	return file_proto_v2_inference_proto_rawDescGZIP(), []int{11}
}

func (x *ModelInferResponse) GetModelName() string {
	if x != nil {
		return x.ModelName
	}
	return ""
}

func (x *ModelInferResponse) GetModelVersion() string {
	if x != nil {
		return x.ModelVersion
	}
	return ""
}

func (x *ModelInferResponse) GetId() string {
	if x != nil {
		return x.Id
	}
	return ""
}

func (x *ModelInferResponse) GetParameters() map[string]*InferParameter {
	if x != nil {
		return x.Parameters
	}
	return nil
}

func (x *ModelInferResponse) GetOutputs() []*ModelInferResponse_InferOutputTensor {
	if x != nil {
		return x.Outputs
	}
	return nil
}

func (x *ModelInferResponse) GetRawOutputContents() [][]byte {
	if x != nil {
		return x.RawOutputContents
	}
	return nil
}

// An inference parameter value. The Parameters message describes a
// “name”/”value” pair, where the “name” is the name of the parameter
// and the “value” is a boolean, integer, or string corresponding to
// the parameter.
type InferParameter struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The parameter value can be a string, an int64, a boolean
	// or a message specific to a predefined parameter.
	//
	// Types that are valid to be assigned to ParameterChoice:
	//
	//	*InferParameter_BoolParam
	//	*InferParameter_Int64Param
	//	*InferParameter_StringParam
	ParameterChoice isInferParameter_ParameterChoice `protobuf_oneof:"parameter_choice"`
	unknownFields   protoimpl.UnknownFields
	sizeCache       protoimpl.SizeCache
}

func (x *InferParameter) Reset() {
	*x = InferParameter{}
	mi := &file_proto_v2_inference_proto_msgTypes[12]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *InferParameter) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*InferParameter) ProtoMessage() {}

func (x *InferParameter) ProtoReflect() protoreflect.Message {
	mi := &file_proto_v2_inference_proto_msgTypes[12]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use InferParameter.ProtoReflect.Descriptor instead.
func (*InferParameter) Descriptor() ([]byte, []int) {
	return file_proto_v2_inference_proto_rawDescGZIP(), []int{12}
}

func (x *InferParameter) GetParameterChoice() isInferParameter_ParameterChoice {
	if x != nil {
		return x.ParameterChoice
	}
	return nil
}

func (x *InferParameter) GetBoolParam() bool {
	if x != nil {
		if x, ok := x.ParameterChoice.(*InferParameter_BoolParam); ok {
			return x.BoolParam
		}
	}
	return false
}

func (x *InferParameter) GetInt64Param() int64 {
	if x != nil {
		if x, ok := x.ParameterChoice.(*InferParameter_Int64Param); ok {
			return x.Int64Param
		}
	}
	return 0
}

func (x *InferParameter) GetStringParam() string {
	if x != nil {
		if x, ok := x.ParameterChoice.(*InferParameter_StringParam); ok {
			return x.StringParam
		}
	}
	return ""
}

type isInferParameter_ParameterChoice interface {
	isInferParameter_ParameterChoice()
}

type InferParameter_BoolParam struct {
	// A boolean parameter value.
	BoolParam bool `protobuf:"varint,1,opt,name=bool_param,json=boolParam,proto3,oneof"`
}

type InferParameter_Int64Param struct {
	// An int64 parameter value.
	Int64Param int64 `protobuf:"varint,2,opt,name=int64_param,json=int64Param,proto3,oneof"`
}

type InferParameter_StringParam struct {
	// A string parameter value.
	StringParam string `protobuf:"bytes,3,opt,name=string_param,json=stringParam,proto3,oneof"`
}

func (*InferParameter_BoolParam) isInferParameter_ParameterChoice() {}

func (*InferParameter_Int64Param) isInferParameter_ParameterChoice() {}

func (*InferParameter_StringParam) isInferParameter_ParameterChoice() {}

// The data contained in a tensor represented by the repeated type
// that matches the tensor's data type. Protobuf oneof is not used
// because oneofs cannot contain repeated fields.
type InferTensorContents struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Representation for BOOL data type. The size must match what is
	// expected by the tensor's shape. The contents must be the flattened,
	// one-dimensional, row-major order of the tensor elements.
	BoolContents []bool `protobuf:"varint,1,rep,packed,name=bool_contents,json=boolContents,proto3" json:"bool_contents,omitempty"`
	// Representation for INT8, INT16, and INT32 data types. The size
	// must match what is expected by the tensor's shape. The contents
	// must be the flattened, one-dimensional, row-major order of the
	// tensor elements.
	IntContents []int32 `protobuf:"varint,2,rep,packed,name=int_contents,json=intContents,proto3" json:"int_contents,omitempty"`
	// Representation for INT64 data types. The size must match what
	// is expected by the tensor's shape. The contents must be the
	// flattened, one-dimensional, row-major order of the tensor elements.
	Int64Contents []int64 `protobuf:"varint,3,rep,packed,name=int64_contents,json=int64Contents,proto3" json:"int64_contents,omitempty"`
	// Representation for UINT8, UINT16, and UINT32 data types. The size
	// must match what is expected by the tensor's shape. The contents
	// must be the flattened, one-dimensional, row-major order of the
	// tensor elements.
	UintContents []uint32 `protobuf:"varint,4,rep,packed,name=uint_contents,json=uintContents,proto3" json:"uint_contents,omitempty"`
	// Representation for UINT64 data types. The size must match what
	// is expected by the tensor's shape. The contents must be the
	// flattened, one-dimensional, row-major order of the tensor elements.
	Uint64Contents []uint64 `protobuf:"varint,5,rep,packed,name=uint64_contents,json=uint64Contents,proto3" json:"uint64_contents,omitempty"`
	// Representation for FP32 data type. The size must match what is
	// expected by the tensor's shape. The contents must be the flattened,
	// one-dimensional, row-major order of the tensor elements.
	Fp32Contents []float32 `protobuf:"fixed32,6,rep,packed,name=fp32_contents,json=fp32Contents,proto3" json:"fp32_contents,omitempty"`
	// Representation for FP64 data type. The size must match what is
	// expected by the tensor's shape. The contents must be the flattened,
	// one-dimensional, row-major order of the tensor elements.
	Fp64Contents []float64 `protobuf:"fixed64,7,rep,packed,name=fp64_contents,json=fp64Contents,proto3" json:"fp64_contents,omitempty"`
	// Representation for BYTES data type. The size must match what is
	// expected by the tensor's shape. The contents must be the flattened,
	// one-dimensional, row-major order of the tensor elements.
	BytesContents [][]byte `protobuf:"bytes,8,rep,name=bytes_contents,json=bytesContents,proto3" json:"bytes_contents,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *InferTensorContents) Reset() {
	*x = InferTensorContents{}
	mi := &file_proto_v2_inference_proto_msgTypes[13]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *InferTensorContents) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*InferTensorContents) ProtoMessage() {}

func (x *InferTensorContents) ProtoReflect() protoreflect.Message {
	mi := &file_proto_v2_inference_proto_msgTypes[13]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use InferTensorContents.ProtoReflect.Descriptor instead.
func (*InferTensorContents) Descriptor() ([]byte, []int) {
	return file_proto_v2_inference_proto_rawDescGZIP(), []int{13}
}

func (x *InferTensorContents) GetBoolContents() []bool {
	if x != nil {
		return x.BoolContents
	}
	return nil
}

func (x *InferTensorContents) GetIntContents() []int32 {
	if x != nil {
		return x.IntContents
	}
	return nil
}

func (x *InferTensorContents) GetInt64Contents() []int64 {
	if x != nil {
		return x.Int64Contents
	}
	return nil
}

func (x *InferTensorContents) GetUintContents() []uint32 {
	if x != nil {
		return x.UintContents
	}
	return nil
}

func (x *InferTensorContents) GetUint64Contents() []uint64 {
	if x != nil {
		return x.Uint64Contents
	}
	return nil
}

func (x *InferTensorContents) GetFp32Contents() []float32 {
	if x != nil {
		return x.Fp32Contents
	}
	return nil
}

func (x *InferTensorContents) GetFp64Contents() []float64 {
	if x != nil {
		return x.Fp64Contents
	}
	return nil
}

func (x *InferTensorContents) GetBytesContents() [][]byte {
	if x != nil {
		return x.BytesContents
	}
	return nil
}

type RepositoryModelLoadRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The name of the model to load, or reload.
	ModelName     string `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *RepositoryModelLoadRequest) Reset() {
	*x = RepositoryModelLoadRequest{}
	mi := &file_proto_v2_inference_proto_msgTypes[14]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *RepositoryModelLoadRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*RepositoryModelLoadRequest) ProtoMessage() {}

func (x *RepositoryModelLoadRequest) ProtoReflect() protoreflect.Message {
	mi := &file_proto_v2_inference_proto_msgTypes[14]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use RepositoryModelLoadRequest.ProtoReflect.Descriptor instead.
func (*RepositoryModelLoadRequest) Descriptor() ([]byte, []int) {
	return file_proto_v2_inference_proto_rawDescGZIP(), []int{14}
}

func (x *RepositoryModelLoadRequest) GetModelName() string {
	if x != nil {
		return x.ModelName
	}
	return ""
}

type RepositoryModelLoadResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The name of the model trying to load or reload.
	ModelName string `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	// boolean parameter to indicate whether model is loaded or not
	IsLoaded      bool `protobuf:"varint,2,opt,name=isLoaded,proto3" json:"isLoaded,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *RepositoryModelLoadResponse) Reset() {
	*x = RepositoryModelLoadResponse{}
	mi := &file_proto_v2_inference_proto_msgTypes[15]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *RepositoryModelLoadResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*RepositoryModelLoadResponse) ProtoMessage() {}

func (x *RepositoryModelLoadResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_v2_inference_proto_msgTypes[15]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use RepositoryModelLoadResponse.ProtoReflect.Descriptor instead.
func (*RepositoryModelLoadResponse) Descriptor() ([]byte, []int) {
	return file_proto_v2_inference_proto_rawDescGZIP(), []int{15}
}

func (x *RepositoryModelLoadResponse) GetModelName() string {
	if x != nil {
		return x.ModelName
	}
	return ""
}

func (x *RepositoryModelLoadResponse) GetIsLoaded() bool {
	if x != nil {
		return x.IsLoaded
	}
	return false
}

type RepositoryModelUnloadRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The name of the model to unload.
	ModelName     string `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *RepositoryModelUnloadRequest) Reset() {
	*x = RepositoryModelUnloadRequest{}
	mi := &file_proto_v2_inference_proto_msgTypes[16]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *RepositoryModelUnloadRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*RepositoryModelUnloadRequest) ProtoMessage() {}

func (x *RepositoryModelUnloadRequest) ProtoReflect() protoreflect.Message {
	mi := &file_proto_v2_inference_proto_msgTypes[16]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use RepositoryModelUnloadRequest.ProtoReflect.Descriptor instead.
func (*RepositoryModelUnloadRequest) Descriptor() ([]byte, []int) {
	return file_proto_v2_inference_proto_rawDescGZIP(), []int{16}
}

func (x *RepositoryModelUnloadRequest) GetModelName() string {
	if x != nil {
		return x.ModelName
	}
	return ""
}

type RepositoryModelUnloadResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The name of the model trying to load or reload.
	ModelName string `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	// boolean parameter to indicate whether model is unloaded or not
	IsUnloaded    bool `protobuf:"varint,2,opt,name=isUnloaded,proto3" json:"isUnloaded,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *RepositoryModelUnloadResponse) Reset() {
	*x = RepositoryModelUnloadResponse{}
	mi := &file_proto_v2_inference_proto_msgTypes[17]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *RepositoryModelUnloadResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*RepositoryModelUnloadResponse) ProtoMessage() {}

func (x *RepositoryModelUnloadResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_v2_inference_proto_msgTypes[17]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use RepositoryModelUnloadResponse.ProtoReflect.Descriptor instead.
func (*RepositoryModelUnloadResponse) Descriptor() ([]byte, []int) {
	return file_proto_v2_inference_proto_rawDescGZIP(), []int{17}
}

func (x *RepositoryModelUnloadResponse) GetModelName() string {
	if x != nil {
		return x.ModelName
	}
	return ""
}

func (x *RepositoryModelUnloadResponse) GetIsUnloaded() bool {
	if x != nil {
		return x.IsUnloaded
	}
	return false
}

// Metadata for a tensor.
type ModelMetadataResponse_TensorMetadata struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The tensor name.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// The tensor data type.
	Datatype string `protobuf:"bytes,2,opt,name=datatype,proto3" json:"datatype,omitempty"`
	// The tensor shape. A variable-size dimension is represented
	// by a -1 value.
	Shape         []int64 `protobuf:"varint,3,rep,packed,name=shape,proto3" json:"shape,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ModelMetadataResponse_TensorMetadata) Reset() {
	*x = ModelMetadataResponse_TensorMetadata{}
	mi := &file_proto_v2_inference_proto_msgTypes[18]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ModelMetadataResponse_TensorMetadata) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ModelMetadataResponse_TensorMetadata) ProtoMessage() {}

func (x *ModelMetadataResponse_TensorMetadata) ProtoReflect() protoreflect.Message {
	mi := &file_proto_v2_inference_proto_msgTypes[18]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ModelMetadataResponse_TensorMetadata.ProtoReflect.Descriptor instead.
func (*ModelMetadataResponse_TensorMetadata) Descriptor() ([]byte, []int) {
	return file_proto_v2_inference_proto_rawDescGZIP(), []int{9, 0}
}

func (x *ModelMetadataResponse_TensorMetadata) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *ModelMetadataResponse_TensorMetadata) GetDatatype() string {
	if x != nil {
		return x.Datatype
	}
	return ""
}

func (x *ModelMetadataResponse_TensorMetadata) GetShape() []int64 {
	if x != nil {
		return x.Shape
	}
	return nil
}

// An input tensor for an inference request.
type ModelInferRequest_InferInputTensor struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The tensor name.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// The tensor data type.
	Datatype string `protobuf:"bytes,2,opt,name=datatype,proto3" json:"datatype,omitempty"`
	// The tensor shape.
	Shape []int64 `protobuf:"varint,3,rep,packed,name=shape,proto3" json:"shape,omitempty"`
	// Optional inference input tensor parameters.
	Parameters map[string]*InferParameter `protobuf:"bytes,4,rep,name=parameters,proto3" json:"parameters,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// The tensor contents using a data-type format. This field must
	// not be specified if "raw" tensor contents are being used for
	// the inference request.
	Contents      *InferTensorContents `protobuf:"bytes,5,opt,name=contents,proto3" json:"contents,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ModelInferRequest_InferInputTensor) Reset() {
	*x = ModelInferRequest_InferInputTensor{}
	mi := &file_proto_v2_inference_proto_msgTypes[19]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ModelInferRequest_InferInputTensor) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ModelInferRequest_InferInputTensor) ProtoMessage() {}

func (x *ModelInferRequest_InferInputTensor) ProtoReflect() protoreflect.Message {
	mi := &file_proto_v2_inference_proto_msgTypes[19]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ModelInferRequest_InferInputTensor.ProtoReflect.Descriptor instead.
func (*ModelInferRequest_InferInputTensor) Descriptor() ([]byte, []int) {
	return file_proto_v2_inference_proto_rawDescGZIP(), []int{10, 0}
}

func (x *ModelInferRequest_InferInputTensor) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *ModelInferRequest_InferInputTensor) GetDatatype() string {
	if x != nil {
		return x.Datatype
	}
	return ""
}

func (x *ModelInferRequest_InferInputTensor) GetShape() []int64 {
	if x != nil {
		return x.Shape
	}
	return nil
}

func (x *ModelInferRequest_InferInputTensor) GetParameters() map[string]*InferParameter {
	if x != nil {
		return x.Parameters
	}
	return nil
}

func (x *ModelInferRequest_InferInputTensor) GetContents() *InferTensorContents {
	if x != nil {
		return x.Contents
	}
	return nil
}

// An output tensor requested for an inference request.
type ModelInferRequest_InferRequestedOutputTensor struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The tensor name.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// Optional requested output tensor parameters.
	Parameters    map[string]*InferParameter `protobuf:"bytes,2,rep,name=parameters,proto3" json:"parameters,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ModelInferRequest_InferRequestedOutputTensor) Reset() {
	*x = ModelInferRequest_InferRequestedOutputTensor{}
	mi := &file_proto_v2_inference_proto_msgTypes[20]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ModelInferRequest_InferRequestedOutputTensor) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ModelInferRequest_InferRequestedOutputTensor) ProtoMessage() {}

func (x *ModelInferRequest_InferRequestedOutputTensor) ProtoReflect() protoreflect.Message {
	mi := &file_proto_v2_inference_proto_msgTypes[20]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ModelInferRequest_InferRequestedOutputTensor.ProtoReflect.Descriptor instead.
func (*ModelInferRequest_InferRequestedOutputTensor) Descriptor() ([]byte, []int) {
	return file_proto_v2_inference_proto_rawDescGZIP(), []int{10, 1}
}

func (x *ModelInferRequest_InferRequestedOutputTensor) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *ModelInferRequest_InferRequestedOutputTensor) GetParameters() map[string]*InferParameter {
	if x != nil {
		return x.Parameters
	}
	return nil
}

// An output tensor returned for an inference request.
type ModelInferResponse_InferOutputTensor struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The tensor name.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// The tensor data type.
	Datatype string `protobuf:"bytes,2,opt,name=datatype,proto3" json:"datatype,omitempty"`
	// The tensor shape.
	Shape []int64 `protobuf:"varint,3,rep,packed,name=shape,proto3" json:"shape,omitempty"`
	// Optional output tensor parameters.
	Parameters map[string]*InferParameter `protobuf:"bytes,4,rep,name=parameters,proto3" json:"parameters,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// The tensor contents using a data-type format. This field must
	// not be specified if "raw" tensor contents are being used for
	// the inference response.
	Contents      *InferTensorContents `protobuf:"bytes,5,opt,name=contents,proto3" json:"contents,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ModelInferResponse_InferOutputTensor) Reset() {
	*x = ModelInferResponse_InferOutputTensor{}
	mi := &file_proto_v2_inference_proto_msgTypes[24]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ModelInferResponse_InferOutputTensor) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ModelInferResponse_InferOutputTensor) ProtoMessage() {}

func (x *ModelInferResponse_InferOutputTensor) ProtoReflect() protoreflect.Message {
	mi := &file_proto_v2_inference_proto_msgTypes[24]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ModelInferResponse_InferOutputTensor.ProtoReflect.Descriptor instead.
func (*ModelInferResponse_InferOutputTensor) Descriptor() ([]byte, []int) {
	return file_proto_v2_inference_proto_rawDescGZIP(), []int{11, 0}
}

func (x *ModelInferResponse_InferOutputTensor) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *ModelInferResponse_InferOutputTensor) GetDatatype() string {
	if x != nil {
		return x.Datatype
	}
	return ""
}

func (x *ModelInferResponse_InferOutputTensor) GetShape() []int64 {
	if x != nil {
		return x.Shape
	}
	return nil
}

func (x *ModelInferResponse_InferOutputTensor) GetParameters() map[string]*InferParameter {
	if x != nil {
		return x.Parameters
	}
	return nil
}

func (x *ModelInferResponse_InferOutputTensor) GetContents() *InferTensorContents {
	if x != nil {
		return x.Contents
	}
	return nil
}

var File_proto_v2_inference_proto protoreflect.FileDescriptor

const file_proto_v2_inference_proto_rawDesc = "" +
	"\n" +
	"\x18proto/v2/inference.proto\x12\tinference\"\x13\n" +
	"\x11ServerLiveRequest\"(\n" +
	"\x12ServerLiveResponse\x12\x12\n" +
	"\x04live\x18\x01 \x01(\bR\x04live\"\x14\n" +
	"\x12ServerReadyRequest\"+\n" +
	"\x13ServerReadyResponse\x12\x14\n" +
	"\x05ready\x18\x01 \x01(\bR\x05ready\"A\n" +
	"\x11ModelReadyRequest\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12\x18\n" +
	"\aversion\x18\x02 \x01(\tR\aversion\"*\n" +
	"\x12ModelReadyResponse\x12\x14\n" +
	"\x05ready\x18\x01 \x01(\bR\x05ready\"\x17\n" +
	"\x15ServerMetadataRequest\"f\n" +
	"\x16ServerMetadataResponse\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12\x18\n" +
	"\aversion\x18\x02 \x01(\tR\aversion\x12\x1e\n" +
	"\n" +
	"extensions\x18\x03 \x03(\tR\n" +
	"extensions\"D\n" +
	"\x14ModelMetadataRequest\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12\x18\n" +
	"\aversion\x18\x02 \x01(\tR\aversion\"\xcf\x02\n" +
	"\x15ModelMetadataResponse\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12\x1a\n" +
	"\bversions\x18\x02 \x03(\tR\bversions\x12\x1a\n" +
	"\bplatform\x18\x03 \x01(\tR\bplatform\x12G\n" +
	"\x06inputs\x18\x04 \x03(\v2/.inference.ModelMetadataResponse.TensorMetadataR\x06inputs\x12I\n" +
	"\aoutputs\x18\x05 \x03(\v2/.inference.ModelMetadataResponse.TensorMetadataR\aoutputs\x1aV\n" +
	"\x0eTensorMetadata\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12\x1a\n" +
	"\bdatatype\x18\x02 \x01(\tR\bdatatype\x12\x14\n" +
	"\x05shape\x18\x03 \x03(\x03R\x05shape\"\x9d\b\n" +
	"\x11ModelInferRequest\x12\x1d\n" +
	"\n" +
	"model_name\x18\x01 \x01(\tR\tmodelName\x12#\n" +
	"\rmodel_version\x18\x02 \x01(\tR\fmodelVersion\x12\x0e\n" +
	"\x02id\x18\x03 \x01(\tR\x02id\x12L\n" +
	"\n" +
	"parameters\x18\x04 \x03(\v2,.inference.ModelInferRequest.ParametersEntryR\n" +
	"parameters\x12E\n" +
	"\x06inputs\x18\x05 \x03(\v2-.inference.ModelInferRequest.InferInputTensorR\x06inputs\x12Q\n" +
	"\aoutputs\x18\x06 \x03(\v27.inference.ModelInferRequest.InferRequestedOutputTensorR\aoutputs\x12,\n" +
	"\x12raw_input_contents\x18\a \x03(\fR\x10rawInputContents\x1a\xcd\x02\n" +
	"\x10InferInputTensor\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12\x1a\n" +
	"\bdatatype\x18\x02 \x01(\tR\bdatatype\x12\x14\n" +
	"\x05shape\x18\x03 \x03(\x03R\x05shape\x12]\n" +
	"\n" +
	"parameters\x18\x04 \x03(\v2=.inference.ModelInferRequest.InferInputTensor.ParametersEntryR\n" +
	"parameters\x12:\n" +
	"\bcontents\x18\x05 \x01(\v2\x1e.inference.InferTensorContentsR\bcontents\x1aX\n" +
	"\x0fParametersEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12/\n" +
	"\x05value\x18\x02 \x01(\v2\x19.inference.InferParameterR\x05value:\x028\x01\x1a\xf3\x01\n" +
	"\x1aInferRequestedOutputTensor\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12g\n" +
	"\n" +
	"parameters\x18\x02 \x03(\v2G.inference.ModelInferRequest.InferRequestedOutputTensor.ParametersEntryR\n" +
	"parameters\x1aX\n" +
	"\x0fParametersEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12/\n" +
	"\x05value\x18\x02 \x01(\v2\x19.inference.InferParameterR\x05value:\x028\x01\x1aX\n" +
	"\x0fParametersEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12/\n" +
	"\x05value\x18\x02 \x01(\v2\x19.inference.InferParameterR\x05value:\x028\x01\"\xdf\x05\n" +
	"\x12ModelInferResponse\x12\x1d\n" +
	"\n" +
	"model_name\x18\x01 \x01(\tR\tmodelName\x12#\n" +
	"\rmodel_version\x18\x02 \x01(\tR\fmodelVersion\x12\x0e\n" +
	"\x02id\x18\x03 \x01(\tR\x02id\x12M\n" +
	"\n" +
	"parameters\x18\x04 \x03(\v2-.inference.ModelInferResponse.ParametersEntryR\n" +
	"parameters\x12I\n" +
	"\aoutputs\x18\x05 \x03(\v2/.inference.ModelInferResponse.InferOutputTensorR\aoutputs\x12.\n" +
	"\x13raw_output_contents\x18\x06 \x03(\fR\x11rawOutputContents\x1a\xd0\x02\n" +
	"\x11InferOutputTensor\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12\x1a\n" +
	"\bdatatype\x18\x02 \x01(\tR\bdatatype\x12\x14\n" +
	"\x05shape\x18\x03 \x03(\x03R\x05shape\x12_\n" +
	"\n" +
	"parameters\x18\x04 \x03(\v2?.inference.ModelInferResponse.InferOutputTensor.ParametersEntryR\n" +
	"parameters\x12:\n" +
	"\bcontents\x18\x05 \x01(\v2\x1e.inference.InferTensorContentsR\bcontents\x1aX\n" +
	"\x0fParametersEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12/\n" +
	"\x05value\x18\x02 \x01(\v2\x19.inference.InferParameterR\x05value:\x028\x01\x1aX\n" +
	"\x0fParametersEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12/\n" +
	"\x05value\x18\x02 \x01(\v2\x19.inference.InferParameterR\x05value:\x028\x01\"\x8d\x01\n" +
	"\x0eInferParameter\x12\x1f\n" +
	"\n" +
	"bool_param\x18\x01 \x01(\bH\x00R\tboolParam\x12!\n" +
	"\vint64_param\x18\x02 \x01(\x03H\x00R\n" +
	"int64Param\x12#\n" +
	"\fstring_param\x18\x03 \x01(\tH\x00R\vstringParamB\x12\n" +
	"\x10parameter_choice\"\xc3\x02\n" +
	"\x13InferTensorContents\x12#\n" +
	"\rbool_contents\x18\x01 \x03(\bR\fboolContents\x12!\n" +
	"\fint_contents\x18\x02 \x03(\x05R\vintContents\x12%\n" +
	"\x0eint64_contents\x18\x03 \x03(\x03R\rint64Contents\x12#\n" +
	"\ruint_contents\x18\x04 \x03(\rR\fuintContents\x12'\n" +
	"\x0fuint64_contents\x18\x05 \x03(\x04R\x0euint64Contents\x12#\n" +
	"\rfp32_contents\x18\x06 \x03(\x02R\ffp32Contents\x12#\n" +
	"\rfp64_contents\x18\a \x03(\x01R\ffp64Contents\x12%\n" +
	"\x0ebytes_contents\x18\b \x03(\fR\rbytesContents\";\n" +
	"\x1aRepositoryModelLoadRequest\x12\x1d\n" +
	"\n" +
	"model_name\x18\x01 \x01(\tR\tmodelName\"X\n" +
	"\x1bRepositoryModelLoadResponse\x12\x1d\n" +
	"\n" +
	"model_name\x18\x01 \x01(\tR\tmodelName\x12\x1a\n" +
	"\bisLoaded\x18\x02 \x01(\bR\bisLoaded\"=\n" +
	"\x1cRepositoryModelUnloadRequest\x12\x1d\n" +
	"\n" +
	"model_name\x18\x01 \x01(\tR\tmodelName\"^\n" +
	"\x1dRepositoryModelUnloadResponse\x12\x1d\n" +
	"\n" +
	"model_name\x18\x01 \x01(\tR\tmodelName\x12\x1e\n" +
	"\n" +
	"isUnloaded\x18\x02 \x01(\bR\n" +
	"isUnloaded2\xd2\x05\n" +
	"\x14GRPCInferenceService\x12K\n" +
	"\n" +
	"ServerLive\x12\x1c.inference.ServerLiveRequest\x1a\x1d.inference.ServerLiveResponse\"\x00\x12N\n" +
	"\vServerReady\x12\x1d.inference.ServerReadyRequest\x1a\x1e.inference.ServerReadyResponse\"\x00\x12K\n" +
	"\n" +
	"ModelReady\x12\x1c.inference.ModelReadyRequest\x1a\x1d.inference.ModelReadyResponse\"\x00\x12W\n" +
	"\x0eServerMetadata\x12 .inference.ServerMetadataRequest\x1a!.inference.ServerMetadataResponse\"\x00\x12T\n" +
	"\rModelMetadata\x12\x1f.inference.ModelMetadataRequest\x1a .inference.ModelMetadataResponse\"\x00\x12K\n" +
	"\n" +
	"ModelInfer\x12\x1c.inference.ModelInferRequest\x1a\x1d.inference.ModelInferResponse\"\x00\x12f\n" +
	"\x13RepositoryModelLoad\x12%.inference.RepositoryModelLoadRequest\x1a&.inference.RepositoryModelLoadResponse\"\x00\x12l\n" +
	"\x15RepositoryModelUnload\x12'.inference.RepositoryModelUnloadRequest\x1a(.inference.RepositoryModelUnloadResponse\"\x00BMZKgithub.com/rbellamy/opentelemetry-inference/metricsinferenceprocessor/protob\x06proto3"

var (
	file_proto_v2_inference_proto_rawDescOnce sync.Once
	file_proto_v2_inference_proto_rawDescData []byte
)

func file_proto_v2_inference_proto_rawDescGZIP() []byte {
	file_proto_v2_inference_proto_rawDescOnce.Do(func() {
		file_proto_v2_inference_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_proto_v2_inference_proto_rawDesc), len(file_proto_v2_inference_proto_rawDesc)))
	})
	return file_proto_v2_inference_proto_rawDescData
}

var file_proto_v2_inference_proto_msgTypes = make([]protoimpl.MessageInfo, 27)
var file_proto_v2_inference_proto_goTypes = []any{
	(*ServerLiveRequest)(nil),                            // 0: inference.ServerLiveRequest
	(*ServerLiveResponse)(nil),                           // 1: inference.ServerLiveResponse
	(*ServerReadyRequest)(nil),                           // 2: inference.ServerReadyRequest
	(*ServerReadyResponse)(nil),                          // 3: inference.ServerReadyResponse
	(*ModelReadyRequest)(nil),                            // 4: inference.ModelReadyRequest
	(*ModelReadyResponse)(nil),                           // 5: inference.ModelReadyResponse
	(*ServerMetadataRequest)(nil),                        // 6: inference.ServerMetadataRequest
	(*ServerMetadataResponse)(nil),                       // 7: inference.ServerMetadataResponse
	(*ModelMetadataRequest)(nil),                         // 8: inference.ModelMetadataRequest
	(*ModelMetadataResponse)(nil),                        // 9: inference.ModelMetadataResponse
	(*ModelInferRequest)(nil),                            // 10: inference.ModelInferRequest
	(*ModelInferResponse)(nil),                           // 11: inference.ModelInferResponse
	(*InferParameter)(nil),                               // 12: inference.InferParameter
	(*InferTensorContents)(nil),                          // 13: inference.InferTensorContents
	(*RepositoryModelLoadRequest)(nil),                   // 14: inference.RepositoryModelLoadRequest
	(*RepositoryModelLoadResponse)(nil),                  // 15: inference.RepositoryModelLoadResponse
	(*RepositoryModelUnloadRequest)(nil),                 // 16: inference.RepositoryModelUnloadRequest
	(*RepositoryModelUnloadResponse)(nil),                // 17: inference.RepositoryModelUnloadResponse
	(*ModelMetadataResponse_TensorMetadata)(nil),         // 18: inference.ModelMetadataResponse.TensorMetadata
	(*ModelInferRequest_InferInputTensor)(nil),           // 19: inference.ModelInferRequest.InferInputTensor
	(*ModelInferRequest_InferRequestedOutputTensor)(nil), // 20: inference.ModelInferRequest.InferRequestedOutputTensor
	nil, // 21: inference.ModelInferRequest.ParametersEntry
	nil, // 22: inference.ModelInferRequest.InferInputTensor.ParametersEntry
	nil, // 23: inference.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry
	(*ModelInferResponse_InferOutputTensor)(nil), // 24: inference.ModelInferResponse.InferOutputTensor
	nil, // 25: inference.ModelInferResponse.ParametersEntry
	nil, // 26: inference.ModelInferResponse.InferOutputTensor.ParametersEntry
}
var file_proto_v2_inference_proto_depIdxs = []int32{
	18, // 0: inference.ModelMetadataResponse.inputs:type_name -> inference.ModelMetadataResponse.TensorMetadata
	18, // 1: inference.ModelMetadataResponse.outputs:type_name -> inference.ModelMetadataResponse.TensorMetadata
	21, // 2: inference.ModelInferRequest.parameters:type_name -> inference.ModelInferRequest.ParametersEntry
	19, // 3: inference.ModelInferRequest.inputs:type_name -> inference.ModelInferRequest.InferInputTensor
	20, // 4: inference.ModelInferRequest.outputs:type_name -> inference.ModelInferRequest.InferRequestedOutputTensor
	25, // 5: inference.ModelInferResponse.parameters:type_name -> inference.ModelInferResponse.ParametersEntry
	24, // 6: inference.ModelInferResponse.outputs:type_name -> inference.ModelInferResponse.InferOutputTensor
	22, // 7: inference.ModelInferRequest.InferInputTensor.parameters:type_name -> inference.ModelInferRequest.InferInputTensor.ParametersEntry
	13, // 8: inference.ModelInferRequest.InferInputTensor.contents:type_name -> inference.InferTensorContents
	23, // 9: inference.ModelInferRequest.InferRequestedOutputTensor.parameters:type_name -> inference.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry
	12, // 10: inference.ModelInferRequest.ParametersEntry.value:type_name -> inference.InferParameter
	12, // 11: inference.ModelInferRequest.InferInputTensor.ParametersEntry.value:type_name -> inference.InferParameter
	12, // 12: inference.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry.value:type_name -> inference.InferParameter
	26, // 13: inference.ModelInferResponse.InferOutputTensor.parameters:type_name -> inference.ModelInferResponse.InferOutputTensor.ParametersEntry
	13, // 14: inference.ModelInferResponse.InferOutputTensor.contents:type_name -> inference.InferTensorContents
	12, // 15: inference.ModelInferResponse.ParametersEntry.value:type_name -> inference.InferParameter
	12, // 16: inference.ModelInferResponse.InferOutputTensor.ParametersEntry.value:type_name -> inference.InferParameter
	0,  // 17: inference.GRPCInferenceService.ServerLive:input_type -> inference.ServerLiveRequest
	2,  // 18: inference.GRPCInferenceService.ServerReady:input_type -> inference.ServerReadyRequest
	4,  // 19: inference.GRPCInferenceService.ModelReady:input_type -> inference.ModelReadyRequest
	6,  // 20: inference.GRPCInferenceService.ServerMetadata:input_type -> inference.ServerMetadataRequest
	8,  // 21: inference.GRPCInferenceService.ModelMetadata:input_type -> inference.ModelMetadataRequest
	10, // 22: inference.GRPCInferenceService.ModelInfer:input_type -> inference.ModelInferRequest
	14, // 23: inference.GRPCInferenceService.RepositoryModelLoad:input_type -> inference.RepositoryModelLoadRequest
	16, // 24: inference.GRPCInferenceService.RepositoryModelUnload:input_type -> inference.RepositoryModelUnloadRequest
	1,  // 25: inference.GRPCInferenceService.ServerLive:output_type -> inference.ServerLiveResponse
	3,  // 26: inference.GRPCInferenceService.ServerReady:output_type -> inference.ServerReadyResponse
	5,  // 27: inference.GRPCInferenceService.ModelReady:output_type -> inference.ModelReadyResponse
	7,  // 28: inference.GRPCInferenceService.ServerMetadata:output_type -> inference.ServerMetadataResponse
	9,  // 29: inference.GRPCInferenceService.ModelMetadata:output_type -> inference.ModelMetadataResponse
	11, // 30: inference.GRPCInferenceService.ModelInfer:output_type -> inference.ModelInferResponse
	15, // 31: inference.GRPCInferenceService.RepositoryModelLoad:output_type -> inference.RepositoryModelLoadResponse
	17, // 32: inference.GRPCInferenceService.RepositoryModelUnload:output_type -> inference.RepositoryModelUnloadResponse
	25, // [25:33] is the sub-list for method output_type
	17, // [17:25] is the sub-list for method input_type
	17, // [17:17] is the sub-list for extension type_name
	17, // [17:17] is the sub-list for extension extendee
	0,  // [0:17] is the sub-list for field type_name
}

func init() { file_proto_v2_inference_proto_init() }
func file_proto_v2_inference_proto_init() {
	if File_proto_v2_inference_proto != nil {
		return
	}
	file_proto_v2_inference_proto_msgTypes[12].OneofWrappers = []any{
		(*InferParameter_BoolParam)(nil),
		(*InferParameter_Int64Param)(nil),
		(*InferParameter_StringParam)(nil),
	}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_proto_v2_inference_proto_rawDesc), len(file_proto_v2_inference_proto_rawDesc)),
			NumEnums:      0,
			NumMessages:   27,
			NumExtensions: 0,
			NumServices:   1,
		},
		GoTypes:           file_proto_v2_inference_proto_goTypes,
		DependencyIndexes: file_proto_v2_inference_proto_depIdxs,
		MessageInfos:      file_proto_v2_inference_proto_msgTypes,
	}.Build()
	File_proto_v2_inference_proto = out.File
	file_proto_v2_inference_proto_goTypes = nil
	file_proto_v2_inference_proto_depIdxs = nil
}
